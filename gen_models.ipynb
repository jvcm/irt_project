{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from irt import IRTModel\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, BayesianRidge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from beta_irt.visualization.plots import newline\n",
    "from beta_irt.visualization.plots import plot_parameters\n",
    "from irt import beta_irt\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from matplotlib import gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import edward as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path\n",
    "path_data = './data/'\n",
    "path_uci = './data/UCI - 45/'\n",
    "\n",
    "# Name of data set\n",
    "name = 'polynomial'\n",
    "\n",
    "# Read csv\n",
    "data = pd.read_csv(path_uci + name + '.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# Parameters\n",
    "rd = 42\n",
    "noise_std = np.linspace(0, 1.0, 20)\n",
    "max_std = noise_std.max()\n",
    "\n",
    "# Variable selection\n",
    "X = data.iloc[:, 0].values.reshape(-1,1)\n",
    "y = data.iloc[:, 1]\n",
    "\n",
    "# Split data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = rd)\n",
    "indexes = list(y_train.index)\n",
    "\n",
    "# # Principal component analysis\n",
    "# pca = PCA(n_components= 1)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "\n",
    "# Standard scale\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "# Regression Models\n",
    "models = [LinearRegression(), BayesianRidge(), svm.SVR(kernel= 'linear'), svm.SVR(kernel = 'rbf', gamma= 'scale', C = 5),\\\n",
    "     KNeighborsRegressor(), DecisionTreeRegressor(), RandomForestRegressor(),\\\n",
    "          AdaBoostRegressor(), MLPRegressor(max_iter=1000, solver= 'lbfgs'), MLPRegressor(hidden_layer_sizes= (50,50), solver = 'lbfgs', max_iter=500, activation='logistic')]\n",
    "\n",
    "# Generate abilities/parameters for BIRT and other info.\n",
    "Irt = IRTModel(models= models)\n",
    "Irt.fit(X_train = X_train, y_train = y_train)\n",
    "\n",
    "# Plot limits\n",
    "xlim = [min(X_test) - 2*max_std, max(X_test) + 2*max_std]\n",
    "ylim = [min(y_test), max(y_test)]\n",
    "\n",
    "# Edward - set seed\n",
    "ed.set_seed(rd)\n",
    "\n",
    "# Folders\n",
    "path = './beta_irt/results/'\n",
    "folder = name + '/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = np.zeros((len(X_test), len(noise_std)))\n",
    "errors = np.zeros((len(noise_std), len(X_test), len(models)))\n",
    "responses = np.zeros((len(noise_std), len(X_test), len(models) + 3))\n",
    "abilities = np.zeros((len(models) + 3, len(noise_std)))\n",
    "params = np.zeros((len(noise_std), len(X_test), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jvcm/irt-env/src/edward/edward/models/random_variable.py:112: Beta.__init__ (from tensorflow.python.ops.distributions.beta) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jvcm/irt-env/lib/python3.6/site-packages/tensorflow/python/ops/distributions/beta.py:208: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jvcm/irt-env/src/edward/edward/models/random_variable.py:112: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jvcm/irt-env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/jvcm/irt_project/beta_irt/models/beta_irt.py:40: Sigmoid.__init__ (from tensorflow.contrib.distributions.python.ops.bijectors.sigmoid) is deprecated and will be removed after 2018-10-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.contrib.distributions`.\n",
      "WARNING:tensorflow:From /home/jvcm/irt-env/src/edward/edward/models/random_variable.py:112: TransformedDistribution.__init__ (from tensorflow.python.ops.distributions.transformed_distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/jvcm/irt-env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 83s | Loss: 0.422\n",
      "None noise is found!\n",
      "None noise is injected!\n",
      "precision 0.0 recall 0.0\n",
      "   1/1000 [  0%]                                ETA: 367s | Loss: 3.948"
     ]
    }
   ],
   "source": [
    "rep = 3\n",
    "for i, noise in enumerate(noise_std):\n",
    "    for itr in range(rep):\n",
    "        # Generate noise to feature in test set\n",
    "        noise_test = np.random.normal(loc=0.0, scale= noise, size= len(X_test))\n",
    "        noises[:, i] += noise_test\n",
    "        X_test_ = X_test + noise_test.reshape(-1,1)\n",
    "\n",
    "        # Generate IRT matrix\n",
    "        Irt.irtMatrix(X_test= X_test_, y_test= y_test, noise_std = i, normalize= True, base_models= True, name= name, rd= rd)\n",
    "        responses[i] += Irt.irt_matrix\n",
    "\n",
    "        name_ = name + '_s' + str(len(y_test)) + '_f' + str(i) + '_sd' + str(rd)\n",
    "\n",
    "        # Generate Items' parameters and Respondents' abilities\n",
    "        os.chdir('./beta_irt/')\n",
    "        %run -i betairt_test.py {'irt_data_'+ name_ +'.csv'}\n",
    "        os.chdir('..')\n",
    "\n",
    "        errors[i] += pd.read_csv('./beta_irt/errors_' + name_ + '.csv').iloc[:, :].values\n",
    "        abilities[:, i] += pd.read_csv(path + folder + 'irt_ability_vi_'+ name_ +'_am1@0_as1@0.csv').iloc[:-1, 1:].values.reshape(1,-1)[0]\n",
    "        params[i] += pd.read_csv(path + folder + 'irt_parameters_vi_'+ name_ +'_am1@0_as1@0.csv').values\n",
    "    \n",
    "    responses[i] /= rep\n",
    "    noises[:, i] /= rep\n",
    "    errors[i] /= rep\n",
    "    abilities[:, i] /= rep\n",
    "    params[i] /= rep\n",
    "    \n",
    "    # Move files to folder    \n",
    "    output = './Results_IRT/'+ folder + 'noise_' + str(i) + '/'\n",
    "    if not os.path.isdir('./Results_IRT/'+ folder):\n",
    "        !mkdir {'./Results_IRT/'+ folder}\n",
    "    if not os.path.isdir(output):\n",
    "        !mkdir {output}\n",
    "        \n",
    "    # ABILITY\n",
    "    pd.DataFrame(data= np.hstack((pd.read_csv(path + folder + 'irt_ability_vi_'+ name_ +'_am1@0_as1@0.csv').iloc[:-1, 0].values.reshape(-1,1),\n",
    "                              abilities[:, i].reshape(-1,1))),\n",
    "             columns = ['Models','Ability']).to_csv(path_or_buf= output + 'irt_ability_vi_'+ name_ + '.csv', index=False)\n",
    "    \n",
    "    # PARAMETERS\n",
    "    pd.DataFrame(data= params[i], columns=['Difficulty','Discrimination']).to_csv(path_or_buf= output + 'irt_parameters_vi_'+ name_ + '.csv', index=False)\n",
    "    \n",
    "    # NOISE\n",
    "    pd.DataFrame(data= noises[:, i].reshape(-1,1), columns=['Noise']).to_csv(path_or_buf= output + 'noise_'+ name_ + '.csv', index=False)\n",
    "    \n",
    "    # ERRORS\n",
    "    pd.DataFrame(data= errors[i], columns=pd.read_csv(path + folder + 'irt_ability_vi_'+ name_ +'_am1@0_as1@0.csv').iloc[:-4, 0].values).to_csv(path_or_buf= output + 'errors_'+ name_ + '.csv', index=False)\n",
    "    \n",
    "    # RESPONSES\n",
    "    pd.DataFrame(data= responses[i], columns=pd.read_csv(path + folder + 'irt_ability_vi_'+ name_ +'_am1@0_as1@0.csv').iloc[:-1, 0].values).to_csv(output + 'irt_data_' + name_ + '.csv', index=False)\n",
    "    \n",
    "!rm {'./beta_irt/*.csv'}\n",
    "!rm {path + folder + '*.csv'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
