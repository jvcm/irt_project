{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from irt import IRTModel\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, BayesianRidge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from beta_irt.visualization.plots import newline\n",
    "from beta_irt.visualization.plots import plot_parameters\n",
    "from irt import beta_irt\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from matplotlib import gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import edward as ed\n",
    "import glob\n",
    "import time, sys\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress, load = 'Progress'):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "#     clear_output(wait = True)\n",
    "    text = load + \": [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text, end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################CREATING ALL REGRESSORS##############################\n",
    "\n",
    "models = [LinearRegression(), BayesianRidge()]\n",
    "names = list(map(lambda x: type(x).__name__, models))\n",
    "\n",
    "C=[1,2,3,4,5]\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "for krn in kernel:\n",
    "    for c in C:\n",
    "        models.append(svm.SVR(kernel= krn, C=c))\n",
    "        names.append('SVR_'+krn[:3] + '_' +str(c))\n",
    "\n",
    "for k in range(2,16):\n",
    "    models.append(KNeighborsRegressor(n_neighbors= k))\n",
    "    names.append('KNR_{}'.format(str(k)))\n",
    "    \n",
    "max_depth = range(2,16)\n",
    "for md in max_depth:\n",
    "    models.append(DecisionTreeRegressor(max_depth = md))\n",
    "    names.append('DT_{}'.format(str(md)))\n",
    "\n",
    "n_estimator = range(20, 60, 10)\n",
    "for md in range(3,12, 2):\n",
    "    for n in n_estimator:\n",
    "        models.append(RandomForestRegressor(n_estimators = n, max_depth = md))\n",
    "        names.append('RF_e{}_md{}'.format(str(n), str(md)))\n",
    "\n",
    "loss = ['linear', 'square', 'exponential']\n",
    "for l in loss:\n",
    "    for n in n_estimator:\n",
    "        models.append(AdaBoostRegressor(n_estimators= n, loss=l))\n",
    "        names.append('AdaB_loss{}_n{}'.format(l[:3], str(n)))\n",
    "\n",
    "archs = [(5,), (10,), (15,), (20,), (30,), (40,), (50,), (60,)]\n",
    "funcs = ['relu', 'logistic']\n",
    "for f in funcs:\n",
    "    for a in archs:\n",
    "        models.append(MLPRegressor(hidden_layer_sizes=a, activation=f))\n",
    "        names.append('MLP_hls{}_f{}' .format(str(a[0]), f[:3]))\n",
    "\n",
    "names = names + ['Average', 'Optimal', 'Worst']\n",
    "        \n",
    "# Parameters\n",
    "rd = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 1 >>>> disclosurez\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 2 >>>> poly5100\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 3 >>>> sin3101\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 4 >>>> bike_sharing_day\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 5 >>>> energy\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 6 >>>> HappinessRank2015\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 7 >>>> poly3101\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 8 >>>> bodyfat\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 9 >>>> sin1100\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 10 >>>> cyclepowerplant\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 11 >>>> balloon\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 12 >>>> disclosurexbias\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 13 >>>> pretinol\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 14 >>>> cpu\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 15 >>>> polynomial\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 16 >>>> ESL\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 17 >>>> kidney\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 18 >>>> sin1101\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 19 >>>> hardware\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 20 >>>> vgalaxy\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 21 >>>> dataset2196cloud\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 22 >>>> cpu\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 23 >>>> chscase_geyser1\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 24 >>>> veteran\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 25 >>>> disclosurexnoise\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 26 >>>> mpg\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 27 >>>> humandevel\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 28 >>>> transplant\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 29 >>>> boston_corrected\n",
      "> Training models\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 30 >>>> places\n",
      "> Training models\n"
     ]
    }
   ],
   "source": [
    "##############################READING ALL DATASETS##############################\n",
    "\n",
    "selected = './data/SELECTED/'\n",
    "dbs = glob.glob(selected + '*.csv')\n",
    "\n",
    "for d, db in enumerate(dbs):\n",
    "    print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "    # Creating folders   \n",
    "    name = db.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    print('Data set ' + str(d + 1) + ' >>>> ' + name)\n",
    "    \n",
    "    if not os.path.isdir('./beta_irt/results/'+ name):\n",
    "        os.system('mkdir ./beta_irt/results/'+ name)\n",
    "    \n",
    "    # Read file\n",
    "    df = pd.read_csv(db, na_values=['?'])\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    #Variable selection\n",
    "    if df.shape[1] > 2:\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        \n",
    "        # Principal component analysis\n",
    "#         pca = PCA(n_components= 1)\n",
    "#         X_train = pca.fit_transform(X_train)\n",
    "#         X_test = pca.transform(X_test)\n",
    "    else: \n",
    "        X = df.iloc[:, 0].values.reshape(-1,1)\n",
    "        y = df.iloc[:, -1].values\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = rd)\n",
    "    \n",
    "    # Standard scale\n",
    "    sc_X = StandardScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "    \n",
    "    sc_y = StandardScaler()\n",
    "    y_train = sc_y.fit_transform(y_train.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    y_test = sc_y.transform(y_test.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    \n",
    "    print('> Training models')\n",
    "    # Generate abilities/parameters for BIRT and other info.\n",
    "    Irt = IRTModel(models= models)\n",
    "    Irt.fit(X_train = X_train, y_train = y_train)\n",
    "\n",
    "    # Folders\n",
    "    path = './beta_irt/results/'\n",
    "    folder = name + '/'\n",
    "\n",
    "    #-------------------------------------Generate-BIRT-------------------------------------#\n",
    "\n",
    "    responses = np.zeros((len(X_test), len(models) + 3))\n",
    "\n",
    "    rep = 40\n",
    "    for itr in range(rep):\n",
    "        # Generate IRT matrix\n",
    "        Irt.irtMatrix(X_test= X_test, y_test= y_test, normalize= True, base_models= True, name= name, rd= rd)\n",
    "        responses += Irt.irt_matrix\n",
    "\n",
    "        name_ = name + '_s' + str(len(y_test)) + '_f0_sd' + str(rd)\n",
    "\n",
    "\n",
    "    responses /= rep\n",
    "\n",
    "    \n",
    "    # Move files to folder    \n",
    "    output = './Results_IRT/IRT_data/'\n",
    "    if not os.path.isdir(output):\n",
    "        os.system('mkdir '+ output)\n",
    "    if not os.path.isdir(output):\n",
    "        os.system('mkdir ' + output)\n",
    "\n",
    "    # RESPONSES\n",
    "    irt_df = pd.DataFrame(data= responses)\n",
    "    irt_df.columns = names\n",
    "    irt_df.to_csv(output + name_ + '.csv', index=False)\n",
    "\n",
    "    #-------------------------------------Clean-Files-------------------------------------#\n",
    "    \n",
    "    os.system('rm ./beta_irt/*.csv')\n",
    "    os.system('rm ' +path + folder + '*.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
