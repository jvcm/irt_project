{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from irt import IRTModel\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, BayesianRidge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from beta_irt.visualization.plots import newline\n",
    "from beta_irt.visualization.plots import plot_parameters\n",
    "from irt import beta_irt\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "from matplotlib import gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import edward as ed\n",
    "import glob\n",
    "import time, sys\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress, load = 'Progress'):\n",
    "    bar_length = 20\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "#     clear_output(wait = True)\n",
    "    text = load + \": [{0}] {1:.1f}%\".format( \"#\" * block + \"-\" * (bar_length - block), progress * 100)\n",
    "    print(text, end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################CREATING ALL REGRESSORS##############################\n",
    "\n",
    "models = [LinearRegression(), BayesianRidge(), svm.SVR(kernel= 'linear'), svm.SVR(kernel = 'rbf', gamma= 'scale', C = 5),\\\n",
    "     KNeighborsRegressor(), DecisionTreeRegressor(), RandomForestRegressor(),\\\n",
    "          AdaBoostRegressor(), MLPRegressor(max_iter=1000, solver= 'lbfgs'), MLPRegressor(hidden_layer_sizes= (50,50), solver = 'lbfgs', max_iter=500, activation='logistic')]\n",
    "\n",
    "names = list(map(lambda x: type(x).__name__, models))\n",
    "names = names + ['Average', 'Optimal', 'Worst']\n",
    "\n",
    "n_synth_models = 3\n",
    "        \n",
    "# Parameters\n",
    "rd = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = './data/SELECTED/'\n",
    "dbs = glob.glob(selected + '*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/SELECTED/disclosurez.csv',\n",
       " './data/SELECTED/poly5100.csv',\n",
       " './data/SELECTED/sin3101.csv',\n",
       " './data/SELECTED/bike_sharing_day.csv',\n",
       " './data/SELECTED/energy.csv',\n",
       " './data/SELECTED/HappinessRank2015.csv',\n",
       " './data/SELECTED/poly3101.csv',\n",
       " './data/SELECTED/bodyfat.csv',\n",
       " './data/SELECTED/sin1100.csv',\n",
       " './data/SELECTED/cyclepowerplant.csv',\n",
       " './data/SELECTED/balloon.csv',\n",
       " './data/SELECTED/disclosurexbias.csv',\n",
       " './data/SELECTED/pretinol.csv',\n",
       " './data/SELECTED/cpu.with.vendor.csv',\n",
       " './data/SELECTED/polynomial.csv',\n",
       " './data/SELECTED/ESL.csv',\n",
       " './data/SELECTED/kidney.csv',\n",
       " './data/SELECTED/sin1101.csv',\n",
       " './data/SELECTED/hardware.csv',\n",
       " './data/SELECTED/vgalaxy.csv',\n",
       " './data/SELECTED/dataset2196cloud.csv',\n",
       " './data/SELECTED/cpu.csv',\n",
       " './data/SELECTED/chscase_geyser1.csv',\n",
       " './data/SELECTED/veteran.csv',\n",
       " './data/SELECTED/disclosurexnoise.csv',\n",
       " './data/SELECTED/mpg.csv',\n",
       " './data/SELECTED/humandevel.csv',\n",
       " './data/SELECTED/transplant.csv',\n",
       " './data/SELECTED/boston_corrected.csv',\n",
       " './data/SELECTED/places.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbs = [\n",
    "    './data/SELECTED/disclosurez.csv',\n",
    "    './data/SELECTED/energy.csv',\n",
    "    './data/SELECTED/cyclepowerplant.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Data set 1 >>>> disclosurez\n",
      "> Training models\n"
     ]
    }
   ],
   "source": [
    "##############################READING ALL DATASETS##############################\n",
    "\n",
    "for d, db in enumerate(dbs):\n",
    "    print('\\n------------------------------------------------------------\\n')\n",
    "\n",
    "    # Creating folders   \n",
    "    name = db.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    print('Data set ' + str(d + 1) + ' >>>> ' + name)\n",
    "    \n",
    "    if not os.path.isdir('./beta_irt/results/'+ name):\n",
    "        os.system('mkdir ./beta_irt/results/'+ name)\n",
    "    \n",
    "    # Read file\n",
    "    df = pd.read_csv(db, na_values=['?'])\n",
    "    df = df.dropna()\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    #Variable selection\n",
    "    if df.shape[1] > 2:\n",
    "        X = df.iloc[:, :-1].values\n",
    "        y = df.iloc[:, -1].values\n",
    "        \n",
    "        # Principal component analysis\n",
    "#         pca = PCA(n_components= 1)\n",
    "#         X_train = pca.fit_transform(X_train)\n",
    "#         X_test = pca.transform(X_test)\n",
    "    else: \n",
    "        X = df.iloc[:, 0].values.reshape(-1,1)\n",
    "        y = df.iloc[:, -1].values\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = rd)\n",
    "    \n",
    "    # Standard scale\n",
    "    sc_X = StandardScaler()\n",
    "    X_train = sc_X.fit_transform(X_train)\n",
    "    X_test = sc_X.transform(X_test)\n",
    "    \n",
    "    sc_y = StandardScaler()\n",
    "    y_train = sc_y.fit_transform(y_train.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    y_test = sc_y.transform(y_test.reshape(-1,1)).reshape(1,-1)[0]\n",
    "    \n",
    "    print('> Training models')\n",
    "    # Generate abilities/parameters for BIRT and other info.\n",
    "    Irt = IRTModel(models= models)\n",
    "    Irt.fit(X_train = X_train, y_train = y_train)\n",
    "\n",
    "    # Noise\n",
    "    rd = 42\n",
    "    noise_std = np.linspace(0, 0.51, 21)\n",
    "    max_std = noise_std.max()\n",
    "    \n",
    "    # Folders\n",
    "    path = './beta_irt/results/'\n",
    "    folder = name + '/'\n",
    "\n",
    "    #-------------------------------------Generate-BIRT-------------------------------------#\n",
    "\n",
    "    rep = 40\n",
    "    for i, noise in enumerate(noise_std):\n",
    "        \n",
    "        noises = np.zeros((len(X_test), len(noise_std)))\n",
    "        errors = np.zeros((len(noise_std), len(X_test), len(models)))\n",
    "        responses = np.zeros((len(noise_std), len(X_test), len(models) + n_synth_models))\n",
    "        abilities = np.zeros((len(models) + n_synth_models, len(noise_std)))\n",
    "        params = np.zeros((len(noise_std), len(X_test), 2))\n",
    "        \n",
    "        name_ = name + '_s' + str(len(y_test)) + '_f' + str(i) + '_sd' + str(rd)\n",
    "        \n",
    "        for itr in range(rep):\n",
    "            # Generate noise to feature in test set\n",
    "#             noise_train = np.random.normal(loc=0.0, scale= noise, size= len(y_train))\n",
    "            noise_test = np.random.normal(loc=0.0, scale= noise, size= len(y_test))\n",
    "\n",
    "#             y_train_ = y_train + noise_train\n",
    "            y_test_ = y_test + noise_test\n",
    "\n",
    "            noises[:, i] += np.absolute(noise_test)\n",
    "\n",
    "            # Generate IRT matrix\n",
    "            Irt.irtMatrix(X_test= X_test, y_test= y_test_, noise_std = i, normalize= True, base_models= True, name= name, rd= rd)\n",
    "            responses[i] += Irt.irt_matrix\n",
    "\n",
    "            \n",
    "\n",
    "            # Generate Items' parameters and Respondents' abilities\n",
    "            os.chdir('./beta_irt/')\n",
    "            os.system('python betairt_test.py irt_data_' + name_ + '.csv')\n",
    "            os.chdir('..')\n",
    "\n",
    "            errors[i] += pd.read_csv('./beta_irt/errors_' + name_ + '.csv').iloc[:, :].values\n",
    "            abilities[:, i] += pd.read_csv(path + folder + 'irt_ability_vi_'+ name_ +'_am1@0_as1@0.csv').iloc[:-1, 1:].values.reshape(1,-1)[0]\n",
    "            params[i] += pd.read_csv(path + folder + 'irt_parameters_vi_'+ name_ +'_am1@0_as1@0.csv').values\n",
    "\n",
    "        responses[i] /= rep\n",
    "        noises[:, i] /= rep\n",
    "        errors[i] /= rep\n",
    "        abilities[:, i] /= rep\n",
    "        params[i] /= rep\n",
    "\n",
    "        # Move files to folder    \n",
    "        output = './Results_IRT/'+ folder + 'noise_' + str(i) + '/'\n",
    "        if not os.path.isdir('./Results_IRT/'+ folder):\n",
    "            os.system('mkdir ./Results_IRT/'+ folder)\n",
    "        if not os.path.isdir(output):\n",
    "            os.system('mkdir ' + output)\n",
    "\n",
    "        # ABILITY\n",
    "        pd.DataFrame(data= np.hstack((pd.read_csv(path + folder + 'irt_ability_vi_'+ name_ +'_am1@0_as1@0.csv').iloc[:-1, 0].values.reshape(-1,1),\n",
    "                                  abilities[:, i].reshape(-1,1))),\n",
    "                 columns = ['Models','Ability']).to_csv(path_or_buf= output + 'irt_ability_vi_'+ name_ + '.csv', index=False)\n",
    "\n",
    "        # PARAMETERS\n",
    "        pd.DataFrame(data= params[i], columns=['Difficulty','Discrimination']).to_csv(path_or_buf= output + 'irt_parameters_vi_'+ name_ + '.csv', index=False)\n",
    "\n",
    "        # NOISE\n",
    "        pd.DataFrame(data= noises[:, i].reshape(-1,1), columns=['Noise']).to_csv(path_or_buf= output + 'noise_'+ name_ + '.csv', index=False)\n",
    "\n",
    "        # ERRORS\n",
    "        pd.DataFrame(data= errors[i], columns=pd.read_csv(path + folder + 'irt_ability_vi_'+ name_ +'_am1@0_as1@0.csv').iloc[:-4, 0].values).to_csv(path_or_buf= output + 'errors_'+ name_ + '.csv', index=False)\n",
    "\n",
    "        # RESPONSES\n",
    "        pd.DataFrame(data= responses[i], columns=pd.read_csv(path + folder + 'irt_ability_vi_'+ name_ +'_am1@0_as1@0.csv').iloc[:-1, 0].values).to_csv(output + 'irt_data_' + name_ + '.csv', index=False)\n",
    "\n",
    "        # X_TEST\n",
    "        pd.DataFrame(data = np.hstack((X_test, y_test_.reshape(-1,1))), columns= ['X_test', 'y_test']).to_csv(path_or_buf= output + 'test_'+ name_ + '.csv', index=False)\n",
    "\n",
    "    #-------------------------------------Clean-Files-------------------------------------#\n",
    "\n",
    "    os.system('rm ./beta_irt/*.csv')\n",
    "    os.system('rm ' +path + folder + '*.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
